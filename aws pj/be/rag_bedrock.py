import boto3
import json
import uuid
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
import numpy as np

class BedrockRAG:
    def __init__(self):
        self.bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
    
    def get_titan_embedding(self, text):
        """L·∫•y embedding t·ª´ Amazon Titan (FREE)"""
        try:
            body = json.dumps({
                "inputText": text
            })
            
            response = self.bedrock_runtime.invoke_model(
                modelId='amazon.titan-embed-text-v1',
                body=body
            )
            
            response_body = json.loads(response['body'].read())
            return response_body['embedding']
            
        except Exception as e:
            print(f"Embedding error: {e}")
            return None
    
    def invoke_claude(self, prompt, max_tokens=1000):
        """G·ªçi Claude cho generation"""
        try:
            body = json.dumps({
                "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
                "max_tokens_to_sample": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
            })
            
            response = self.bedrock_runtime.invoke_model(
                modelId='anthropic.claude-instant-v1',
                body=body
            )
            
            response_body = json.loads(response['body'].read())
            return response_body['completion']
            
        except Exception as e:
            print(f"Claude error: {e}")
            return None
    
    def invoke_titan(self, prompt, max_tokens=1000):
        """G·ªçi Amazon Titan cho generation (FREE)"""
        try:
            body = json.dumps({
                "inputText": prompt,
                "textGenerationConfig": {
                    "maxTokenCount": max_tokens,
                    "temperature": 0.7,
                    "topP": 0.9,
                }
            })
            
            response = self.bedrock_runtime.invoke_model(
                modelId='amazon.titan-text-lite-v1',
                body=body
            )
            
            response_body = json.loads(response['body'].read())
            return response_body['results'][0]['outputText']
            
        except Exception as e:
            print(f"Titan error: {e}")
            return None
    
    def load_and_split_document(self, file_path):
        """Load v√† chia nh·ªè document"""
        print(f"üìñ Loading document: {file_path}")
        
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        else:
            loader = TextLoader(file_path, encoding='utf-8')
        
        docs = loader.load()
        chunks = self.text_splitter.split_documents(docs)
        print(f"üìÑ Split into {len(chunks)} chunks")
        return chunks
    
    def create_vector_store(self, chunks):
        """T·∫°o vector store v·ªõi Titan embeddings"""
        print("üîÑ Creating vector store v·ªõi Titan embeddings...")
        
        # L·∫•y embeddings cho m·ªói chunk
        texts = [chunk.page_content for chunk in chunks]
        embeddings = []
        
        for i, text in enumerate(texts):
            if i % 5 == 0:  # Log progress every 5 chunks
                print(f"üìä Processing chunk {i+1}/{len(texts)}")
            
            embedding = self.get_titan_embedding(text)
            if embedding:
                embeddings.append(embedding)
            else:
                # Fallback: zero vector
                embeddings.append([0] * 1536)
        
        # T·∫°o FAISS index
        import faiss
        dimension = len(embeddings[0]) if embeddings else 1536
        index = faiss.IndexFlatL2(dimension)
        
        if embeddings:
            index.add(np.array(embeddings))
        
        return {
            'index': index,
            'chunks': chunks,
            'texts': texts,
            'embeddings': embeddings
        }
    
    def similarity_search(self, vector_store, query, k=3):
        """T√¨m c√°c chunk li√™n quan nh·∫•t"""
        # L·∫•y embedding cho query
        query_embedding = self.get_titan_embedding(query)
        if not query_embedding:
            return []
        
        # T√¨m ki·∫øm similarity
        distances, indices = vector_store['index'].search(
            np.array([query_embedding]), k
        )
        
        # L·∫•y c√°c chunk li√™n quan
        relevant_chunks = []
        for idx in indices[0]:
            if idx < len(vector_store['chunks']):
                relevant_chunks.append(vector_store['chunks'][idx])
        
        return relevant_chunks
    
    def answer_question(self, vector_store, question):
        """Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n RAG"""
        print(f"üîç Searching for relevant content...")
        
        # T√¨m c√°c chunk li√™n quan
        relevant_chunks = self.similarity_search(vector_store, question, k=3)
        
        if not relevant_chunks:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."
        
        # X√¢y d·ª±ng context
        context = "\n\n".join([
            f"ƒêo·∫°n {i+1}: {chunk.page_content}" 
            for i, chunk in enumerate(relevant_chunks)
        ])
        
        # T·∫°o prompt cho RAG
        prompt = f"""
        H√£y ƒë·ªçc k·ªπ c√°c ƒëo·∫°n vƒÉn b·∫£n sau t·ª´ t√†i li·ªáu:

        {context}

        D·ª±a TR√äN c√°c ƒëo·∫°n vƒÉn b·∫£n tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau:
        C√¢u h·ªèi: {question}

        Y√™u c·∫ßu:
        - Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c ƒëo·∫°n vƒÉn b·∫£n tr√™n
        - N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ
        - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† chi ti·∫øt

        Tr·∫£ l·ªùi:
        """
        
        print("ü§ñ Generating answer with Bedrock...")
        
        # D√πng Claude ho·∫∑c Titan ƒë·ªÉ generate
        answer = self.invoke_claude(prompt)
        if not answer:
            answer = self.invoke_titan(prompt)
        
        return answer or "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."

# Global instance
bedrock_rag = BedrockRAG()